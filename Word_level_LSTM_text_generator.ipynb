{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Word-level LSTM text generator from twitter.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "metadata": {
        "id": "iZpuGb_MF94Z",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Develop Word-Level Language Model"
      ]
    },
    {
      "metadata": {
        "id": "4zFGb4AoM3Vb",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Import libraries"
      ]
    },
    {
      "metadata": {
        "id": "RKDa5GYwHxhd",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from pickle import dump,load\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from keras.layers import Embedding, LSTM, Dense\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.callbacks import EarlyStopping\n",
        "from keras.models import Sequential,load_model\n",
        "from keras.utils.np_utils import to_categorical\n",
        "import numpy as np\n",
        "import io"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "pZhLRQANHNJx",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Reading the dataset"
      ]
    },
    {
      "metadata": {
        "id": "8Mt9G6UUvhL9",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "!wget https://raw.githubusercontent.com/ichaparroc/Word-Level-Neural-Languaje-Model-for-Generate-Text/master/spanish_emojis5.csv"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "ACLflZu-HQks",
        "colab_type": "code",
        "outputId": "fef66417-313b-492c-e822-42252e8cea37",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "link=\"spanish_emojis5.csv\"\n",
        "with io.open(link,encoding='utf-8') as f:\n",
        "  data=f.read().lower() #.replace('\\n',' \\n ')\n",
        "print('Corpus lenght in characters:',len(data))"
      ],
      "execution_count": 180,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Corpus lenght in characters: 44428\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "8TwSf0W8p_wU",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Tokenization"
      ]
    },
    {
      "metadata": {
        "id": "096xAwy4Mg1z",
        "colab_type": "code",
        "outputId": "475c442b-c538-42c4-8429-3b669b512d01",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "tokenizer=Tokenizer()\n",
        "corpus=data.lower().split(\"\\n\")    \n",
        "tokenizer.fit_on_texts(corpus)\n",
        "total_words=len(tokenizer.word_index) + 1\n",
        "print('Total words: ',total_words)"
      ],
      "execution_count": 141,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Total words:  2518\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "Y7G2n_MJqFGv",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Generate Sequences"
      ]
    },
    {
      "metadata": {
        "id": "dX6hQEROLOZq",
        "colab_type": "code",
        "outputId": "b92280b2-ac3d-4bc1-98a2-6fcced614e10",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "input_sequences=[]\n",
        "for line in corpus:\n",
        "  token_list=tokenizer.texts_to_sequences([line])[0]\n",
        "  for i in range(1,len(token_list)):\n",
        "    n_gram_sequence=token_list[:i+1]\n",
        "    input_sequences.append(n_gram_sequence)\n",
        "print('Total Sequences:',len(input_sequences))"
      ],
      "execution_count": 142,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Total Sequences: 7651\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "l4_d-qPvqV1c",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Pre-Padding Sequences"
      ]
    },
    {
      "metadata": {
        "id": "zr6yiZa6ORdF",
        "colab_type": "code",
        "outputId": "15cd5b2a-45ad-4e9e-deab-a8d37bff7e3d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "max_sequence_len=max([len(x) for x in input_sequences])\n",
        "input_sequences=np.array(pad_sequences(input_sequences,maxlen=max_sequence_len,padding='pre'))\n",
        "print('Max Sequence Length:',max_sequence_len)"
      ],
      "execution_count": 177,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Max Sequence Length: 30\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "PJzSY8pbqacb",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Divide in X (inputs) and y (outpus - *onehot encoding*)"
      ]
    },
    {
      "metadata": {
        "id": "jTZ3lk4EamHj",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "sequences=np.array(input_sequences)\n",
        "X,y=sequences[:,:-1],sequences[:,-1]\n",
        "y=to_categorical(y,num_classes=total_words,dtype='int8')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "B7a_1IffqlcB",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Define Model RNN/LSTM"
      ]
    },
    {
      "metadata": {
        "id": "fAkrhgulZZfy",
        "colab_type": "code",
        "outputId": "632399c7-b654-420a-d35d-cb766c32e042",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 260
        }
      },
      "cell_type": "code",
      "source": [
        "model=Sequential()\n",
        "model.add(Embedding(total_words,10,input_length=max_sequence_len-1))\n",
        "model.add(LSTM(50))\n",
        "model.add(Dense(total_words,activation='softmax'))\n",
        "print(model.summary())\n",
        "\n",
        "#model = Sequential()\n",
        "#model.add(Embedding(vocab_size, 50, input_length=seq_length))\n",
        "#model.add(LSTM(100, return_sequences=True))\n",
        "#model.add(LSTM(100))\n",
        "#model.add(Dense(100, activation='relu'))\n",
        "#model.add(Dense(vocab_size, activation='softmax'))"
      ],
      "execution_count": 145,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding_7 (Embedding)      (None, 29, 10)            25180     \n",
            "_________________________________________________________________\n",
            "lstm_7 (LSTM)                (None, 50)                12200     \n",
            "_________________________________________________________________\n",
            "dense_7 (Dense)              (None, 2518)              128418    \n",
            "=================================================================\n",
            "Total params: 165,798\n",
            "Trainable params: 165,798\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "xcQ0Atc5qpTh",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Compile Model with Categorial Loss"
      ]
    },
    {
      "metadata": {
        "id": "BssS_Tr4TvCr",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "fqvqxHCnqwZ5",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Fit Model"
      ]
    },
    {
      "metadata": {
        "id": "HU8V_kT5_po-",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "\"\"\"\"You will get different results, but perhaps an accuracy of just over **50%** of predicting the next word in the sequence, which is not bad. We are not aiming for 100% accuracy (e.g. a model that memorized the text), but rather a model that captures the essence of the text.\"\"\"\"\n",
        "\n",
        "https://machinelearningmastery.com/how-to-develop-a-word-level-neural-language-model-in-keras/"
      ]
    },
    {
      "metadata": {
        "id": "Kl5K8y8mT5sL",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "model.fit(X,y,epochs=350,verbose=1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "KurqfvJ-qx6p",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Save Model"
      ]
    },
    {
      "metadata": {
        "id": "VNglWIL7pqUb",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "model.save('model_5.h5')\n",
        "dump(tokenizer,open('tokenizer_5.pkl','wb'))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "cGx3ZNnWrFZv",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Generate Text using Neural Language Model"
      ]
    },
    {
      "metadata": {
        "id": "2tjQPc2DrMEU",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Load Model"
      ]
    },
    {
      "metadata": {
        "id": "0RetZqL7wYhY",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "!wget https://raw.githubusercontent.com/ichaparroc/Word-Level-Neural-Languaje-Model-for-Generate-Text/master/model_5.h5\n",
        "!wget https://raw.githubusercontent.com/ichaparroc/Word-Level-Neural-Languaje-Model-for-Generate-Text/master/tokenizer_5.pkl"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "VDRdsfk5BekL",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "model = load_model('model_5.h5')\n",
        "tokenizer = load(open('tokenizer_5.pkl','rb'))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "rF7OmfT7rOBY",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Define \"Recursive\" Function For Generate Text"
      ]
    },
    {
      "metadata": {
        "id": "r2yE4lV_BIQZ",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def generate_text(seed_text,next_words,max_sequence_len, model):\n",
        "  for j in range(next_words):\n",
        "    token_list = tokenizer.texts_to_sequences([seed_text])[0]\n",
        "    token_list = pad_sequences([token_list], maxlen=max_sequence_len-1, padding='pre')\n",
        "    predicted = model.predict_classes(token_list, verbose=0)\n",
        "    output_word = \"\"\n",
        "    for word, index in tokenizer.word_index.items():\n",
        "      if index == predicted:\n",
        "        output_word = word\n",
        "        break\n",
        "    seed_text += \" \" + output_word\n",
        "  return seed_text"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "hhO_MWXRrSrj",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Enjoy The Generation Of Text :D"
      ]
    },
    {
      "metadata": {
        "id": "9P-Qzf2QJOlr",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "1d4c7aa9-6804-44b5-fd14-a9bd2dbfe50c"
      },
      "cell_type": "code",
      "source": [
        "text=generate_text(\"en la calle\",25,max_sequence_len,model)\n",
        "print(text)"
      ],
      "execution_count": 185,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "en la calle pera 1000 anos en septiembre la historias de mi casa y me equivoque creo que se beba la vida del estudiante que algun 13 octobre\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "MQqLq8acJoKf",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "bdbb3d5a-734e-4720-b355-b89de7b1f638"
      },
      "cell_type": "code",
      "source": [
        "text=generate_text(\"todos vamos a\",25,max_sequence_len,model)\n",
        "print(text)"
      ],
      "execution_count": 150,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "todos vamos a la 9 am a la cama y tampoco esta que me duele el huevo me creen que la verdaderos blink siempre solo a 100 con\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "1O6PbMh3olJS",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "039493b5-0344-4531-bedc-27a1b465a33e"
      },
      "cell_type": "code",
      "source": [
        "text=generate_text(\"estoy viendo\",25,max_sequence_len,model)\n",
        "print(text)"
      ],
      "execution_count": 152,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "estoy viendo el dia re raro no fui del colectivo a la casa me bianca gente lo mas bueno que te ventilador pensados bien o me duele\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "2PAkb0M0om-L",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "1c521918-ede8-46f6-cfc9-f58a0bc0bac8"
      },
      "cell_type": "code",
      "source": [
        "text=generate_text(\"cada vez que\",25,max_sequence_len,model)\n",
        "print(text)"
      ],
      "execution_count": 153,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "cada vez que madrugo mucho al dia mas huevos que la cube para confirmar la maleta participaron en el palo en camiseta de 091 en leroy que buenos\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "161wk970oshl",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "223820a0-e30e-4b8b-b760-98545269e72b"
      },
      "cell_type": "code",
      "source": [
        "text=generate_text(\"como no\",25,max_sequence_len,model)\n",
        "print(text)"
      ],
      "execution_count": 159,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "como no me maltraten el pelo morado y evoluciona a turquesa en do suceda de parecer un completo alcoholico amaia y alfred estoy despierta a la 430\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "QI6zuU-_o8-2",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "1b3036c0-8556-43fa-c17e-ae9d38be3d66"
      },
      "cell_type": "code",
      "source": [
        "text=generate_text(\"sigo en\",25,max_sequence_len,model)\n",
        "print(text)"
      ],
      "execution_count": 164,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "sigo en la mejor godo de la casa y me voy a la casa y me encanta la vuelta siempre me encanta la vuelta siempre me da\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "IU4D0FVWpSN0",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "d0aa1ad6-8202-4537-ba91-37ee07018461"
      },
      "cell_type": "code",
      "source": [
        "text=generate_text(\"estoy con\",25,max_sequence_len,model)\n",
        "print(text)"
      ],
      "execution_count": 176,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "estoy con la mina mas linda que me duele el huevo me entretiene cereales internacionales era postureo y creo que cambio cada 2x3 sistersorpresa miercoles ven a\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "45fa5FDDpfHf",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}